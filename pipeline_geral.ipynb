{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c15be329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00833b9",
   "metadata": {},
   "source": [
    "## Ingesting data and Extracting DataFrame\n",
    "\n",
    "As requested the operational and sales head insert all the operations and sales inside a XLS documentation.\n",
    "\n",
    "This document is hand-craft and may contain errors. We will iterate.\n",
    "\n",
    "Before this Pipeline we are generated we already had made previous test in a Test Scale in the Company's SandBox.\n",
    "\n",
    "In the previous tests we gathered that the steps to clean and organize data is:\n",
    "\n",
    "* Add header and convert to snake_case;\n",
    "\n",
    "* First column —> Pedido —> make it a header\n",
    "\n",
    "* Delete columns that have more than 10% of null values\n",
    "\n",
    "* Delete rows with all null values\n",
    "\n",
    "\n",
    "In this Script we will make a ETL process Where:\n",
    "\n",
    "- Ingestion --> From local Computer\n",
    "\n",
    "- Extraction --> From the several Sheets on the XLS file\n",
    "\n",
    "- Transformation --> We will work on the cleaning and organizing data for de Data Analysis team\n",
    "\n",
    "- Load --> Load will be as one single csv file to a blob storage.\n",
    "\n",
    "\n",
    "As we are creating a automated pipeline we will need to overwrite our Blob monthly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c107d7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path of the Excel file\n",
    "file_path = '/Users/gabrielreus/Desktop/TAZ/ML_DA/jun-2023/databases/database_sales.xls'\n",
    "\n",
    "# Get the current year\n",
    "current_year = datetime.now().year\n",
    "\n",
    "# Create an empty list to store the DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Iterate over the years\n",
    "for year in range(2017, current_year + 1):\n",
    "    # Convert the year to string\n",
    "    year_str = str(year)\n",
    "    \n",
    "    # Load the sheet corresponding to the year as a DataFrame\n",
    "    df = pd.read_excel(file_path, sheet_name=year_str)\n",
    "    \n",
    "    # Add the DataFrame to the list\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68670fd",
   "metadata": {},
   "source": [
    "# Analyzing the DataFrames\n",
    "\n",
    "We created a list of dataframes from the XLS doc.\n",
    "\n",
    "We will now start pre analyze data, to confirm its correct format. \n",
    "\n",
    "First we will test it out to verify if the list generated is equal the actual year dif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0d279f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "# Verify if it worked all right and stoping if needed\n",
    "\n",
    "try:\n",
    "    if (current_year - 2017) == len(dfs) - 1:\n",
    "        print('Success')\n",
    "    else:\n",
    "        print(\"An error occurred\")\n",
    "        sys.exit(\"Stopping the script.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    sys.exit(\"Stopping the script.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd5d4e5",
   "metadata": {},
   "source": [
    "# Transformation\n",
    "\n",
    "We will start our steps to transform the data we have to something clean, organized and readble to a structured table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "13d07033",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in enumerate(dfs):\n",
    "    # Encontrar a linha que contém 'pedido' na primeira coluna\n",
    "    linha_pedido = x[x.iloc[:, 0].astype(str).str.contains('pedido', case=False, na=False)].index[0]\n",
    "\n",
    "    # Definir a linha encontrada como o cabeçalho do DataFrame\n",
    "    x.columns = x.iloc[linha_pedido]\n",
    "\n",
    "    # Converter o cabeçalho para snake_case\n",
    "    x.columns = x.columns.str.lower().str.replace(' ', '_')\n",
    "\n",
    "    # Remover a linha que foi utilizada como cabeçalho\n",
    "    x = x.drop(x.index[linha_pedido])\n",
    "\n",
    "    # Redefinir o índice do DataFrame\n",
    "    x = x.reset_index(drop=True)\n",
    "\n",
    "    # Limpar rótulos de colunas vazios ou NaN\n",
    "    x.columns = x.columns.fillna('')\n",
    "\n",
    "    # Calcular a porcentagem de valores nulos em cada coluna\n",
    "    porcentagem_nulos = x.isnull().sum() / len(x) * 100\n",
    "\n",
    "    # Obter a lista de colunas que possuem mais de 50% de valores nulos\n",
    "    colunas_a_remover = porcentagem_nulos[porcentagem_nulos > 50].index\n",
    "\n",
    "    # Verificar se há colunas a serem removidas\n",
    "    if len(colunas_a_remover) > 0:\n",
    "        # Remover as colunas com mais de 50% de valores nulos\n",
    "        x = x.drop(columns=colunas_a_remover)\n",
    "\n",
    "    # Definir a porcentagem mínima de valores não nulos para manter a linha\n",
    "    porcentagem_minima = 80\n",
    "\n",
    "    # Calcular a quantidade mínima de valores não nulos necessários\n",
    "    quantidade_minima = len(x.columns) * (porcentagem_minima / 100)\n",
    "\n",
    "    # Remover linhas com mais de 50% de valores nulos\n",
    "    x = x.dropna(thresh=quantidade_minima)\n",
    "\n",
    "    # Update the corresponding DataFrame in the dfs list\n",
    "    dfs[i] = x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "894bc90e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-210f074b92da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dfs' is not defined"
     ]
    }
   ],
   "source": [
    "dfs[1].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965ae11c",
   "metadata": {},
   "source": [
    "## Check on the list of Dataframes generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ab26777b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All DataFrames have the number of columns.\n"
     ]
    }
   ],
   "source": [
    "# Check if all DataFrames have the same number of columns\n",
    "num_columns = len(column_names)\n",
    "same_num_columns = all(len(df.columns) == num_columns for df in dfs)\n",
    "\n",
    "# Check if number conditions are True\n",
    "if same_num_columns:\n",
    "    print(\"All DataFrames have the number of columns.\")\n",
    "else:\n",
    "    print(\"DataFrames do not have the same name and number of columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887a6d01",
   "metadata": {},
   "source": [
    "## Rename all the columns name with the first DF pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5ca21241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the column names from the first DataFrame in the list\n",
    "column_names = dfs[0].columns\n",
    "\n",
    "# Rename the columns of all DataFrames in the list\n",
    "for i, df in enumerate(dfs):\n",
    "    dfs[i] = df.set_axis(column_names, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a3b9ab",
   "metadata": {},
   "source": [
    "## Join all of the DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d9e70ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join DataFrames on index\n",
    "merged_df = pd.concat(dfs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "81cd6ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 20010 entries, 2 to 3157\n",
      "Data columns (total 21 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   pedido       20008 non-null  object\n",
      " 1   data_compra  20004 non-null  object\n",
      " 2   empresa      20007 non-null  object\n",
      " 3   produtos     20010 non-null  object\n",
      " 4   quantidade   20008 non-null  object\n",
      " 5   valor        20010 non-null  object\n",
      " 6   icms         20007 non-null  object\n",
      " 7   pis/cofins   20003 non-null  object\n",
      " 8   ipi          20005 non-null  object\n",
      " 9   custo_net    20010 non-null  object\n",
      " 10  form.pgto    19953 non-null  object\n",
      " 11  data_saída   20005 non-null  object\n",
      " 12  empresa      20010 non-null  object\n",
      " 13  valor        20010 non-null  object\n",
      " 14  venda_net    20010 non-null  object\n",
      " 15  rep.1_(%)    20009 non-null  object\n",
      " 16  comissão     20010 non-null  object\n",
      " 17  líquida      20010 non-null  object\n",
      " 18  líquido_(%)  20010 non-null  object\n",
      " 19  saldo_net    20010 non-null  object\n",
      " 20  saldo        20010 non-null  object\n",
      "dtypes: object(21)\n",
      "memory usage: 3.4+ MB\n"
     ]
    }
   ],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76aee2e",
   "metadata": {},
   "source": [
    "## Final clean of the merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "26796260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 20004 entries, 2 to 3157\n",
      "Data columns (total 21 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   pedido       20004 non-null  object\n",
      " 1   data_compra  20004 non-null  object\n",
      " 2   empresa      20003 non-null  object\n",
      " 3   produtos     20004 non-null  object\n",
      " 4   quantidade   20004 non-null  object\n",
      " 5   valor        20004 non-null  object\n",
      " 6   icms         20001 non-null  object\n",
      " 7   pis/cofins   19997 non-null  object\n",
      " 8   ipi          19999 non-null  object\n",
      " 9   custo_net    20004 non-null  object\n",
      " 10  form.pgto    19947 non-null  object\n",
      " 11  data_saída   19999 non-null  object\n",
      " 12  empresa      20004 non-null  object\n",
      " 13  valor        20004 non-null  object\n",
      " 14  venda_net    20004 non-null  object\n",
      " 15  rep.1_(%)    20003 non-null  object\n",
      " 16  comissão     20004 non-null  object\n",
      " 17  líquida      20004 non-null  object\n",
      " 18  líquido_(%)  20004 non-null  object\n",
      " 19  saldo_net    20004 non-null  object\n",
      " 20  saldo        20004 non-null  object\n",
      "dtypes: object(21)\n",
      "memory usage: 3.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# Remove NaN values from the \"data_compra\" column\n",
    "merged_df.dropna(subset=['data_compra'], inplace=True)\n",
    "\n",
    "# Show\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc10d46",
   "metadata": {},
   "source": [
    "In this pipeline we will not work on the Dtype. We will leave it to the DA / DS to do it so. Due to different approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b036728",
   "metadata": {},
   "source": [
    "# Load\n",
    "\n",
    "We will load it in two different ways. \n",
    "\n",
    "1) On the root\n",
    "\n",
    "2) Direct to our Azure SQL Server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3601187b",
   "metadata": {},
   "source": [
    "## 1) Direct on the root - Databricks directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6da754ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a CSV file from the DataFrame\n",
    "data_gen = merged_df.to_csv('dados.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7883669f",
   "metadata": {},
   "source": [
    "## 2) Direct to DB - Azure SQL Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7cb53447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Library\n",
    "import pyodbc\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "582cb512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure SQL server details\n",
    "\n",
    "server = 'totalingredientes.database.windows.net'\n",
    "database = 'totaling_brazil'\n",
    "username = 'your_username'\n",
    "password = 'your_password'\n",
    "table_name = 'operations-by-year'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dec9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the connection string\n",
    "conn_str = f'DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password}'\n",
    "\n",
    "# Establish a connection to the Azure SQL server\n",
    "conn = pyodbc.connect(conn_str)\n",
    "\n",
    "# Create a cursor object to execute SQL queries\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321ced94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate the existing table to remove all data\n",
    "truncate_query = f'TRUNCATE TABLE {table_name}'\n",
    "cursor.execute(truncate_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4bba75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an SQLAlchemy engine using the connection\n",
    "engine = create_engine(f'mssql+pyodbc:///?odbc_connect={conn_str}')\n",
    "\n",
    "# Upload the DataFrame to the Azure SQL server table\n",
    "merged_df.to_sql(table_name, con=engine, if_exists='replace', index=False)\n",
    "\n",
    "# Close the cursor and connection\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"DF data uploaded to Azure SQL server successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
